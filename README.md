# Simple Pearson Correlation Using Map/Reduce

This project demonstrates a very simple map/reduce example to compute the Pearson correlation coefficients for columns in a table. The project uses the [mincemeat map/reduce library for Python](https://github.com/michaelfairley/mincemeatpy) in order to show how the data pipeline works.

The map and reduce functions are inspired by (but corrected) [Computing Pearson Correlation using Hadoopâ€™s Map/Reduce (M/R) Paradigm](https://vangjee.wordpress.com/2012/02/29/computing-pearson-correlation-using-hadoops-mapreduce-mr-paradigm/).

## Overview

I'm using a very simple dataset in CSV format which contains a header row and several data rows. Each data row contains an identifier and several values. We want to ignore the identifier and find correlations between the remaining rows.


## Computing Pearson Correlation Coefficient

The Pearson correlation coefficient between two columns is computed using the following algorithm (note that this contains redundant parenthesis for clarity):

    numerator =
        sum of products - (product of sums / count)
    denominator =
        ( (sum of x^2) - (sum of x)^2 / count) ^ 2
        *
        ( (sum of y^2) - (sum of y)^2 / count) ^ 2
    result =
        numerator / denominator

A sample R script has been included along with the resulting output to see the target values we are trying to compute. For this small dataset even R is overkill, but this helps us validate that our algorithm computes the right answers. See `correlate.r` and `correlate.r.Rout` along with `data.csv` to view our expected answers.


## Reading the Data

Load the CSV file. Parse the header row for column names separate out the data rows.

    with open('data.csv') as infile:
        lines = infile.read().split('\n')
        headers = lines[0].split(',')
        lines = [ line.split(',') for line in lines[1:-1] ]
    
    datasource = { line[0]: [ float(x) for x in line[1:] ] for line in lines }


## Mapper

The mapper receives rows from the dataset and emits one key:value pair for each combination of values in that row. The key for each output pair is the pair of indices of the values, and the value is the pair of values themselves. For instance, if the row in question contains 7 in position 1 and 3 in position 4 then there should be one row emitted with key `(1, 4)` (the indices) and value
`(7, 3)`.

Since row `p1` contains 1, 1, 3, and -1, the following key:value pairs should be emitted by the mapper for this row:

    (0,1): (1,1)
    (0,2): (1,3)
    (0,3): (1,-1)
    (1,2): (1,3)
    (1,3): (1,-1)
    (2,3): (3,-1)

Note that order doesn't matter, we simply enumerate them in this order so that it's easy to write and easy to verify.

## Reducer

The reducer receives key:value-list pairs for the results of the map function. In our case, the reducer will be called once for each unique index pair generated by the mappers and the value list for each of these will be a list of all the value pairs associated with that index pair. In this way we are receiving two columns as pairwise rows down the table.

The reducer simply follows the formula given earlier to compute the coefficient for those two rows and outputs the coefficient as a single value.


## Using Mincemeat

The `mincemeat` library for Python makes testing distributed map/reduce jobs much easier, as it takes care of all the plumbing for us allowing us to focus on just the mapper and reducer function.

To run the server (our script):

    python <scriptname>

To run the worker(s):

    python mincemeat.py -p <password> <hostname>

Since we're running everthing on localhost and this computes extremely fast for our small dataset, we only need to run these two lines (each in separate terminal windows):

    python correlate.py
    python mincemeat.py -p changeme localhost

The output will be pairs of columns by name (`a`, `b`, `c`, and `d`) and the correlation coefficient between those two columns.
